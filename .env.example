# LLM Generation Configuration
# Copy this file to .env and fill in your API keys

# ============================================================================
# LLM Provider Selection
# ============================================================================
# Options: openai, anthropic, google, local
LLM_PROVIDER=openai

# ============================================================================
# OpenAI Configuration
# ============================================================================
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4
# Alternatives: gpt-3.5-turbo, gpt-4-turbo, gpt-4o

# OpenAI Advanced Settings
OPENAI_TEMPERATURE=0.1
OPENAI_MAX_TOKENS=2000
OPENAI_TOP_P=1.0

# ============================================================================
# Anthropic (Claude) Configuration
# ============================================================================
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229
# Alternatives: claude-3-opus-20240229, claude-3-haiku-20240307, claude-2.1

# Anthropic Advanced Settings
ANTHROPIC_TEMPERATURE=0.1
ANTHROPIC_MAX_TOKENS=2000

# ============================================================================
# Google Gemini Configuration
# ============================================================================
GOOGLE_API_KEY=your-google-api-key-here
GOOGLE_MODEL=gemini-pro
# Alternatives: gemini-pro, gemini-1.5-pro, gemini-1.5-flash

# Google Advanced Settings
GOOGLE_TEMPERATURE=0.1
GOOGLE_MAX_TOKENS=2000
GOOGLE_TOP_P=1.0

# ============================================================================
# Local Model Configuration (OpenAI-compatible API)
# ============================================================================
LOCAL_API_BASE=http://localhost:8000/v1
LOCAL_MODEL=local-model-name
LOCAL_API_KEY=not-needed-for-local

# ============================================================================
# Generation Settings
# ============================================================================

# Citation Enforcement
REQUIRE_CITATIONS=true
STRICT_GROUNDING=true

# Response Constraints
DISCOURAGE_SPECULATION=true
FORCE_REFUSAL_IF_UNCERTAIN=true

# Code Validation (VTK API MCP)
VALIDATE_CODE=false
# Options: true/false - Enable post-generation VTK code validation

VALIDATION_MAX_RETRIES=1
# Options: 0-5 - Number of correction attempts if validation fails
# 0 = validation disabled
# 1 = single correction attempt (recommended)
# 2-5 = multiple attempts (may increase cost/latency)

# Logging
LOG_LEVEL=INFO
LOG_REQUESTS=true
LOG_RESPONSES=true

# Retry Settings
MAX_RETRIES=3
RETRY_DELAY=1

# ============================================================================
# Notes
# ============================================================================
# - Keep your API keys secure! Never commit .env to version control
# - Temperature: Lower (0-0.3) = more focused, Higher (0.7-1.0) = more creative
# - For VTK RAG, recommend temperature=0.1 for accuracy
# - Set STRICT_GROUNDING=true to enforce "no answer if not in context"
#
# ============================================================================
# Other LLM Providers (Not Yet Implemented)
# ============================================================================
# The following providers could be added in the future:
# - Cohere (via cohere SDK)
# - Mistral AI (via mistral SDK)
# - Together AI (via together SDK)
# - Replicate (via replicate SDK)  
# - Hugging Face Inference API
#
# For now, many of these can be accessed via LOCAL provider if they offer
# an OpenAI-compatible API endpoint.
